# cf_ai_agents_chat


An AI-powered chat application built using **Cloudflare Agents**, **Workers AI (Llama 3.3)**, and **Durable Object–backed memory**.


---

## **1. Features**

### Real-time chat with an AI assistant

Powered by Cloudflare Workers AI (Llama 3.3), with streaming responses.

### Stateful memory

Agent (a Durable Object under the hood) stores previous messages, allowing context-aware responses.

### Cloudflare-native architecture

Uses Workers, Durable Objects, Agents, and Workers AI.



## **2. Architecture Overview**

```
┌──────────────┐       ┌────────────────────┐       ┌─────────────────────────┐
│ User Browser │  ---> │ Worker + Agent     │ --->  │ Workers AI (Llama 3.3)  │
│ (React chat) │       │ (Durable Object)   │       │ Model inference         │
└──────────────┘       └────────────────────┘       └─────────────────────────┘
        ↑                       │
        └─────── streamText <───┘
```

### Components:

| Component      | Description                                    |
| -------------- | ---------------------------------------------- |
| **Frontend**   | React/Vite chat UI (auto-generated by starter) |
| **Agent**      | Durable Object storing conversation state      |
| **Workers AI** | Runs Llama-3.3-70B at the edge                 |
| **Wrangler**   | Build + deploy tool                            |

---

## **3. Running the App Locally**

### **Prerequisites**

* **Node.js 20.19+ or 22.12+**
* **npm** or **pnpm**
* **Cloudflare account**
* **Wrangler CLI**

Install Wrangler:

```bash
npm install -g wrangler
wrangler login
```

### **Run locally**

From inside the generated app folder:

```bash
npm install
npm run dev
```

When Wrangler asks:

```
Run in local or remote mode? (l/r)
```

Press:

```
l + Enter
```

### **Open the UI**

Vite will report something like:

```
Local: http://localhost:5173/
```

Open that URL.





